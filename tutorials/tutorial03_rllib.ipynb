{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 03: Running RLlib Experiments\n",
    "\n",
    "This tutorial walks you through the process of running traffic simulations in Flow with trainable RLlib-powered agents. Autonomous agents will learn to maximize a certain reward over the rollouts, using the [**RLlib**](https://ray.readthedocs.io/en/latest/rllib.html) library ([citation](https://arxiv.org/abs/1712.09381)) ([installation instructions](https://flow.readthedocs.io/en/latest/flow_setup.html#optional-install-ray-rllib)). Simulations of this form will depict the propensity of RL agents to influence the traffic of a human fleet in order to make the whole fleet more efficient (for some given metrics). \n",
    "\n",
    "In this tutorial, we simulate an initially perturbed single lane ring road, where we introduce a single autonomous vehicle. We witness that, after some training, that the autonomous vehicle learns to dissipate the formation and propagation of \"phantom jams\" which form when only human driver dynamics are involved.\n",
    "\n",
    "## 1. Components of a Simulation\n",
    "All simulations, both in the presence and absence of RL, require two components: a *network*, and an *environment*. Networks describe the features of the transportation network used in simulation. This includes the positions and properties of nodes and edges constituting the lanes and junctions, as well as properties of the vehicles, traffic lights, inflows, etc... in the network. Environments, on the other hand, initialize, reset, and advance simulations, and act as the primary interface between the reinforcement learning algorithm and the network. Moreover, custom environments may be used to modify the dynamical features of an network. Finally, in the RL case, it is in the *environment* that the state/action spaces and the reward function are defined. \n",
    "\n",
    "## 2. Setting up a Network\n",
    "Flow contains a plethora of pre-designed networks used to replicate highways, intersections, and merges in both closed and open settings. All these networks are located in flow/networks. For this tutorial, which involves a single lane ring road, we will use the network `RingNetwork`.\n",
    "\n",
    "### 2.1 Setting up Network Parameters\n",
    "\n",
    "The network mentioned at the start of this section, as well as all other networks in Flow, are parameterized by the following arguments: \n",
    "* name\n",
    "* vehicles\n",
    "* net_params\n",
    "* initial_config\n",
    "\n",
    "These parameters are explained in detail in `tutorial01_sumo.ipynb`. Moreover, all parameters excluding vehicles (covered in section 2.2) do not change from the previous tutorial. Accordingly, we specify them nearly as we have before, and leave further explanations of the parameters to `tutorial01_sumo.ipynb`.\n",
    "\n",
    "We begin by choosing the network the experiment will be trained on. We use one of Flow's builtin networks, located in `flow.networks`. A list of all available networks can be found by running the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Network', 'BayBridgeNetwork', 'BayBridgeTollNetwork', 'BottleneckNetwork', 'FigureEightNetwork', 'TrafficLightGridNetwork', 'HighwayNetwork', 'RingNetwork', 'MergeNetwork', 'MultiRingNetwork', 'MiniCityNetwork', 'HighwayRampsNetwork', 'I210SubNetwork']\n"
     ]
    }
   ],
   "source": [
    "import flow.networks as networks\n",
    "\n",
    "print(networks.__all__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we choose to use the ring road network. The network class is then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.networks import RingNetwork\n",
    "\n",
    "# ring road network class\n",
    "network_name = RingNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One key difference between SUMO and RLlib experiments is that, in RLlib experiments, the network classes do not need to be defined; instead users should simply name the network class they wish to use. Later on, an environment setup module will import the correct network class based on the provided names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameter classes to the network class\n",
    "from flow.core.params import NetParams, InitialConfig\n",
    "\n",
    "# name of the network\n",
    "name = \"training_example\"\n",
    "\n",
    "# network-specific parameters\n",
    "from flow.networks.ring import ADDITIONAL_NET_PARAMS\n",
    "net_params = NetParams(additional_params=ADDITIONAL_NET_PARAMS)\n",
    "\n",
    "# initial configuration to vehicles\n",
    "initial_config = InitialConfig(spacing=\"uniform\", perturbation=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Adding Trainable Autonomous Vehicles\n",
    "The `Vehicles` class stores state information on all vehicles in the network. This class is used to identify the dynamical features of a vehicle and whether it is controlled by a reinforcement learning agent. Morover, information pertaining to the observations and reward function can be collected from various `get` methods within this class.\n",
    "\n",
    "The dynamics of vehicles in the `Vehicles` class can either be depicted by sumo or by the dynamical methods located in flow/controllers. For human-driven vehicles, we use the IDM model for acceleration behavior, with exogenous gaussian acceleration noise with std 0.2 m/s2 to induce perturbations that produce stop-and-go behavior. In addition, we use the `ContinousRouter` routing controller so that the vehicles may maintain their routes closed networks.\n",
    "\n",
    "As we have done in `tutorial01_sumo.ipynb`, human-driven vehicles are defined in the `VehicleParams` class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vehicles class\n",
    "from flow.core.params import VehicleParams\n",
    "\n",
    "# vehicles dynamics models\n",
    "from flow.controllers import IDMController, ContinuousRouter\n",
    "\n",
    "vehicles = VehicleParams()\n",
    "vehicles.add(\"human\",\n",
    "             acceleration_controller=(IDMController, {}),\n",
    "             routing_controller=(ContinuousRouter, {}),\n",
    "             num_vehicles=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above addition to the `Vehicles` class only accounts for 21 of the 22 vehicles that are placed in the network. We now add an additional trainable autuonomous vehicle whose actions are dictated by an RL agent. This is done by specifying an `RLController` as the acceleraton controller to the vehicle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.controllers import RLController"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this controller serves primarirly as a placeholder that marks the vehicle as a component of the RL agent, meaning that lane changing and routing actions can also be specified by the RL agent to this vehicle.\n",
    "\n",
    "We finally add the vehicle as follows, while again using the `ContinuousRouter` to perpetually maintain the vehicle within the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles.add(veh_id=\"rl\",\n",
    "             acceleration_controller=(RLController, {}),\n",
    "             routing_controller=(ContinuousRouter, {}),\n",
    "             num_vehicles=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting up an Environment\n",
    "\n",
    "Several environments in Flow exist to train RL agents of different forms (e.g. autonomous vehicles, traffic lights) to perform a variety of different tasks. The use of an environment allows us to view the cumulative reward simulation rollouts receive, along with to specify the state/action spaces.\n",
    "\n",
    "Sumo envrionments in Flow are parametrized by three components:\n",
    "* `SumoParams`\n",
    "* `EnvParams`\n",
    "* `Network`\n",
    "\n",
    "### 3.1 SumoParams\n",
    "`SumoParams` specifies simulation-specific variables. These variables include the length of any simulation step and whether to render the GUI when running the experiment. For this example, we consider a simulation step length of 0.1s and deactivate the GUI. \n",
    "\n",
    "**Note** For training purposes, it is highly recommanded to deactivate the GUI in order to avoid global slow down. In such case, one just needs to specify the following: `render=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import SumoParams\n",
    "\n",
    "sim_params = SumoParams(sim_step=0.1, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 EnvParams\n",
    "\n",
    "`EnvParams` specifies environment and experiment-specific parameters that either affect the training process or the dynamics of various components within the network. For the environment `WaveAttenuationPOEnv`, these parameters are used to dictate bounds on the accelerations of the autonomous vehicles, as well as the range of ring lengths (and accordingly network densities) the agent is trained on.\n",
    "\n",
    "Finally, it is important to specify here the *horizon* of the experiment, which is the duration of one episode (during which the RL-agent acquire data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import EnvParams\n",
    "\n",
    "# Define horizon as a variable to ensure consistent use across notebook\n",
    "HORIZON=100\n",
    "\n",
    "env_params = EnvParams(\n",
    "    # length of one rollout\n",
    "    horizon=HORIZON,\n",
    "\n",
    "    additional_params={\n",
    "        # maximum acceleration of autonomous vehicles\n",
    "        \"max_accel\": 1,\n",
    "        # maximum deceleration of autonomous vehicles\n",
    "        \"max_decel\": 1,\n",
    "        # bounds on the ranges of ring road lengths the autonomous vehicle \n",
    "        # is trained on\n",
    "        \"ring_length\": [220, 270],\n",
    "        \"target_velocity\": 10,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Initializing a Gym Environment\n",
    "\n",
    "Now, we have to specify our Gym Environment and the algorithm that our RL agents will use. Similar to the network, we choose to use on of Flow's builtin environments, a list of which is provided by the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Env', 'AccelEnv', 'LaneChangeAccelEnv', 'LaneChangeAccelPOEnv', 'TrafficLightGridTestEnv', 'MergePOEnv', 'BottleneckEnv', 'BottleneckAccelEnv', 'WaveAttenuationEnv', 'WaveAttenuationPOEnv', 'TrafficLightGridEnv', 'TrafficLightGridPOEnv', 'TrafficLightGridBenchmarkEnv', 'BottleneckDesiredVelocityEnv', 'TestEnv', 'BayBridgeEnv', 'BottleNeckAccelEnv', 'DesiredVelocityEnv', 'PO_TrafficLightGridEnv', 'GreenWaveTestEnv']\n"
     ]
    }
   ],
   "source": [
    "import flow.envs as flowenvs\n",
    "\n",
    "print(flowenvs.__all__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the environment \"WaveAttenuationPOEnv\", which is used to train autonomous vehicles to attenuate the formation and propagation of waves in a partially observable variable density ring road. To create the Gym Environment, the only necessary parameters are the environment name plus the previously defined variables. These are defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from flow.envs import MergePOEnv\n",
    "\n",
    "env_name = MergePOEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Setting up Flow Parameters\n",
    "\n",
    "RLlib experiments both generate a `params.json` file for each experiment run. For RLlib experiments, the parameters defining the Flow network and environment must be stored as well. As such, in this section we define the dictionary `flow_params`, which contains the variables required by the utility function `make_create_env`. `make_create_env` is a higher-order function which returns a function `create_env` that initializes a Gym environment corresponding to the Flow network specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating flow_params. Make sure the dictionary keys are as specified. \n",
    "flow_params = dict(\n",
    "    # name of the experiment\n",
    "    exp_tag=name,\n",
    "    # name of the flow environment the experiment is running on\n",
    "    env_name=env_name,\n",
    "    # name of the network class the experiment uses\n",
    "    network=network_name,\n",
    "    # simulator that is used by the experiment\n",
    "    simulator='traci',\n",
    "    # simulation-related parameters\n",
    "    sim=sim_params,\n",
    "    # environment related parameters (see flow.core.params.EnvParams)\n",
    "    env=env_params,\n",
    "    # network-related parameters (see flow.core.params.NetParams and\n",
    "    # the network's documentation or ADDITIONAL_NET_PARAMS component)\n",
    "    net=net_params,\n",
    "    # vehicles to be placed in the network at the start of a rollout \n",
    "    # (see flow.core.vehicles.Vehicles)\n",
    "    veh=vehicles,\n",
    "    # (optional) parameters affecting the positioning of vehicles upon \n",
    "    # initialization/reset (see flow.core.params.InitialConfig)\n",
    "    initial=initial_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym==0.14.0\n",
      "gym-notices==0.0.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze | grep gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Running RL experiments in Ray\n",
    "\n",
    "### 4.1 Import \n",
    "\n",
    "First, we must import modules required to run experiments in Ray. The `json` package is required to store the Flow experiment parameters in the `params.json` file, as is `FlowParamsEncoder`. Ray-related imports are required: the PPO algorithm agent, `ray.tune`'s experiment runner, and environment helper methods `register_env` and `make_create_env`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Initializing Ray\n",
    "Here, we initialize Ray and experiment-based constant variables specifying parallelism in the experiment as well as experiment batch size in terms of number of rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:05:57,717\tINFO worker.py:963 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.8.13', ray_version='1.12.0', ray_commit='f18fc31c7562990955556899090f8e8656b48d2d', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-04-22_16-55-02_052174_61272/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-04-22_16-55-02_052174_61272/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2022-04-22_16-55-02_052174_61272', 'metrics_export_port': 61387, 'gcs_address': '127.0.0.1:59346', 'address': '127.0.0.1:59346', 'node_id': '0d9e7bb48f33ca5afcf80b53ecf2a89c50b28fb6b4447fc55bdb07e0'})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parallel workers\n",
    "N_CPUS = 2\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 1\n",
    "\n",
    "ray.init(num_cpus=N_CPUS, ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Configuration and Setup\n",
    "Here, we copy and modify the default configuration for the [PPO algorithm](https://arxiv.org/abs/1707.06347). The agent has the number of parallel workers specified, a batch size corresponding to `N_ROLLOUTS` rollouts (each of which has length `HORIZON` steps), a discount rate $\\gamma$ of 0.999, two hidden layers of size 16, uses Generalized Advantage Estimation, $\\lambda$ of 0.97, and other parameters as set below.\n",
    "\n",
    "Once `config` contains the desired parameters, a JSON string corresponding to the `flow_params` specified in section 3 is generated. The `FlowParamsEncoder` maps objects to string representations so that the experiment can be reproduced later. That string representation is stored within the `env_config` section of the `config` dictionary. Later, `config` is written out to the file `params.json`. \n",
    "\n",
    "Next, we call `make_create_env` and pass in the `flow_params` to return a function we can use to register our Flow environment with Gym. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm or model to train. This may refer to \"\n",
    "#      \"the name of a built-on algorithm (e.g. RLLib's DQN \"\n",
    "#      \"or PPO), or a user-defined trainable function or \"\n",
    "#      \"class registered in the tune registry.\")\n",
    "alg_run = \"PPO\"\n",
    "\n",
    "agent_cls = get_agent_class(alg_run)\n",
    "config = agent_cls._default_config.copy()\n",
    "config[\"num_workers\"] = N_CPUS - 1  # number of parallel workers\n",
    "config[\"train_batch_size\"] = HORIZON * N_ROLLOUTS  # batch size\n",
    "config[\"gamma\"] = 0.999  # discount rate\n",
    "config[\"model\"].update({\"fcnet_hiddens\": [16, 16]})  # size of hidden layers in network\n",
    "config[\"use_gae\"] = True  # using generalized advantage estimation\n",
    "config[\"lambda\"] = 0.97  \n",
    "config[\"sgd_minibatch_size\"] = min(16 * 1024, config[\"train_batch_size\"])  # stochastic gradient descent\n",
    "config[\"kl_target\"] = 0.02  # target KL divergence\n",
    "config[\"num_sgd_iter\"] = 10  # number of SGD iterations\n",
    "config[\"horizon\"] = HORIZON  # rollout horizon\n",
    "\n",
    "config[\"framework\"] = 'torch'\n",
    "\n",
    "# save the flow params for replay\n",
    "flow_json = json.dumps(flow_params, cls=FlowParamsEncoder, sort_keys=True,\n",
    "                       indent=4)  # generating a string version of flow_params\n",
    "config['env_config']['flow_params'] = flow_json  # adding the flow_params to config dict\n",
    "config['env_config']['run'] = alg_run\n",
    "\n",
    "# Call the utility function make_create_env to be able to \n",
    "# register the Flow env for this experiment\n",
    "create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "# Register as rllib env with Gym\n",
    "register_env(gym_name, create_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Running Experiments\n",
    "\n",
    "Here, we use the `run_experiments` function from `ray.tune`. The function takes a dictionary with one key, a name corresponding to the experiment, and one value, itself a dictionary containing parameters for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:06:05,293\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m 2022-04-22 17:06:08,404\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m 2022-04-22 17:06:08,404\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m 2022-04-22 17:06:08,405\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:06:11 (running for 00:00:07.05)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:06:11,521\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:06:11,523\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:06:11 (running for 00:00:07.06)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">           1</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m 2022-04-22 17:06:11,517\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=63979, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=63979, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=63986, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f976c6d8160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63979)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=63986)\u001b[0m 2022-04-22 17:06:11,505\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=63986, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f976c6d8160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=63986)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=63986)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=63986)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=63986)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=63986)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=63986)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=63986)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=63986)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=63986)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=63986)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=63986)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=63986)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=63986)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:06:12,573\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m 2022-04-22 17:06:15,914\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m 2022-04-22 17:06:15,915\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m 2022-04-22 17:06:15,915\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m 2022-04-22 17:06:18,789\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=63995, ip=127.0.0.1, repr=PPOTrainer)"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:06:18 (running for 00:00:14.32)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">           1</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=63995, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64037, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f80a43b0160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63995)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64037)\u001b[0m 2022-04-22 17:06:18,775\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64037, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f80a43b0160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64037)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64037)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64037)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64037)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64037)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64037)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64037)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64037)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64037)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64037)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64037)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64037)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64037)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:06:18,796\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:06:18,799\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "2022-04-22 17:06:19,548\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m 2022-04-22 17:06:22,826\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m 2022-04-22 17:06:22,826\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m 2022-04-22 17:06:22,826\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:06:25 (running for 00:00:21.51)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">           2</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:06:25,978\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:06:25,989\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m 2022-04-22 17:06:25,973\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64044, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64044, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64048, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fddcc4c7160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64044)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64048)\u001b[0m 2022-04-22 17:06:25,957\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64048, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fddcc4c7160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64048)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64048)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64048)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64048)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64048)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64048)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64048)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64048)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64048)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64048)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64048)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64048)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64048)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:06:27,699\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m 2022-04-22 17:06:31,259\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m 2022-04-22 17:06:31,259\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m 2022-04-22 17:06:31,259\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:06:34 (running for 00:00:29.92)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">           3</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:06:34,391\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:06:34,400\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m 2022-04-22 17:06:34,386\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64059, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64059, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64063, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fbe3c01f160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64059)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64063)\u001b[0m 2022-04-22 17:06:34,373\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64063, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fbe3c01f160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64063)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64063)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64063)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64063)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64063)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64063)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64063)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64063)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64063)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64063)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64063)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64063)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64063)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:06:35,586\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m 2022-04-22 17:06:38,588\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m 2022-04-22 17:06:38,588\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m 2022-04-22 17:06:38,589\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:06:41 (running for 00:00:37.11)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">           4</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:06:41,581\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:06:41,582\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m 2022-04-22 17:06:41,576\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64069, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64069, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64072, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f842c9cf160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64069)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64072)\u001b[0m 2022-04-22 17:06:41,564\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64072, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f842c9cf160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64072)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64072)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64072)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64072)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64072)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64072)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64072)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64072)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64072)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64072)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64072)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64072)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64072)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:06:42,634\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m 2022-04-22 17:06:45,517\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m 2022-04-22 17:06:45,518\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m 2022-04-22 17:06:45,518\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:06:48 (running for 00:00:44.15)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">           5</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:06:48,622\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:06:48,627\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m 2022-04-22 17:06:48,617\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64078, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64078, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64082, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7facdba5f160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64078)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64082)\u001b[0m 2022-04-22 17:06:48,604\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64082, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7facdba5f160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64082)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64082)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64082)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64082)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64082)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64082)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64082)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64082)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64082)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64082)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64082)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64082)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64082)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:06:49,734\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m 2022-04-22 17:06:53,208\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m 2022-04-22 17:06:53,208\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m 2022-04-22 17:06:53,208\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:06:56 (running for 00:00:51.72)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">           6</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:06:56,195\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:06:56,201\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m 2022-04-22 17:06:56,191\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64087, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64087, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64095, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fd65c2df160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64087)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64095)\u001b[0m 2022-04-22 17:06:56,179\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64095, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fd65c2df160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64095)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64095)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64095)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64095)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64095)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64095)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64095)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64095)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64095)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64095)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64095)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64095)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64095)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:06:57,744\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m 2022-04-22 17:07:00,779\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m 2022-04-22 17:07:00,779\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m 2022-04-22 17:07:00,779\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:07:04 (running for 00:00:59.63)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">           7</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:07:04,102\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:07:04,103\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m 2022-04-22 17:07:04,094\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64101, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64101, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64140, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fd6fbbc7160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64101)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64140)\u001b[0m 2022-04-22 17:07:04,079\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64140, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fd6fbbc7160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64140)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64140)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64140)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64140)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64140)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64140)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64140)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64140)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64140)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64140)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64140)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64140)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64140)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:07:05,558\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m 2022-04-22 17:07:08,765\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m 2022-04-22 17:07:08,765\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m 2022-04-22 17:07:08,765\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:07:12 (running for 00:01:07.64)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">           8</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:07:12,117\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:07:12,121\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m 2022-04-22 17:07:12,112\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64154, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64154, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64158, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fdc7c343160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64154)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64158)\u001b[0m 2022-04-22 17:07:12,097\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64158, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fdc7c343160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64158)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64158)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64158)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64158)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64158)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64158)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64158)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64158)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64158)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64158)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64158)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64158)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64158)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:07:13,741\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m 2022-04-22 17:07:16,992\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m 2022-04-22 17:07:16,992\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m 2022-04-22 17:07:16,992\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:07:20 (running for 00:01:15.69)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">           9</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:07:20,167\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:07:20,173\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m 2022-04-22 17:07:20,162\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64165, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64165, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64172, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fce83cb0100>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64165)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64172)\u001b[0m 2022-04-22 17:07:20,149\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64172, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fce83cb0100>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64172)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64172)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64172)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64172)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64172)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64172)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64172)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64172)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64172)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64172)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64172)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64172)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64172)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:07:21,666\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m 2022-04-22 17:07:24,819\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m 2022-04-22 17:07:24,819\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m 2022-04-22 17:07:24,819\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:07:28 (running for 00:01:24.07)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          10</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:07:28,539\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:07:28,546\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m 2022-04-22 17:07:28,534\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64178, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64178, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64182, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fc54ba72160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64178)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64182)\u001b[0m 2022-04-22 17:07:28,519\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64182, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fc54ba72160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64182)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64182)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64182)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64182)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64182)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64182)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64182)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64182)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64182)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64182)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64182)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64182)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64182)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:07:29,904\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m 2022-04-22 17:07:33,153\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m 2022-04-22 17:07:33,153\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m 2022-04-22 17:07:33,153\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:07:36 (running for 00:01:32.09)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          11</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:07:36,562\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:07:36,565\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m 2022-04-22 17:07:36,555\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64190, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64190, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64194, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fbb6adbf160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64190)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64194)\u001b[0m 2022-04-22 17:07:36,544\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64194, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fbb6adbf160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64194)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64194)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64194)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64194)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64194)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64194)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64194)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64194)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64194)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64194)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64194)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64194)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64194)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:07:37,707\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m 2022-04-22 17:07:41,094\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m 2022-04-22 17:07:41,094\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m 2022-04-22 17:07:41,094\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m 2022-04-22 17:07:44,293\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64201, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64201, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64204, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7ff598623160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64201)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64204)\u001b[0m 2022-04-22 17:07:44,280\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64204, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7ff598623160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64204)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64204)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64204)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64204)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64204)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64204)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64204)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64204)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64204)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64204)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64204)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64204)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64204)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:07:44 (running for 00:01:39.83)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          12</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:07:44,299\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:07:44,307\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "2022-04-22 17:07:45,725\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m 2022-04-22 17:07:48,829\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m 2022-04-22 17:07:48,829\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m 2022-04-22 17:07:48,829\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m 2022-04-22 17:07:51,988\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64241, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64241, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64249, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f8648df7160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64241)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64249)\u001b[0m 2022-04-22 17:07:51,977\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64249, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f8648df7160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64249)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64249)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64249)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64249)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64249)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64249)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64249)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64249)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64249)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64249)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64249)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64249)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64249)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:07:51 (running for 00:01:47.52)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          13</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:07:51,995\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:07:51,998\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "2022-04-22 17:07:53,649\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m 2022-04-22 17:07:57,037\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m 2022-04-22 17:07:57,037\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m 2022-04-22 17:07:57,037\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:08:00 (running for 00:01:55.82)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          14</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:08:00,296\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:08:00,297\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m 2022-04-22 17:08:00,291\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64256, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64256, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64293, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f77c3df2160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64256)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64293)\u001b[0m 2022-04-22 17:08:00,276\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64293, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f77c3df2160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64293)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64293)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64293)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64293)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64293)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64293)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64293)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64293)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64293)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64293)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64293)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64293)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64293)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:08:01,676\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m 2022-04-22 17:08:04,681\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m 2022-04-22 17:08:04,681\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m 2022-04-22 17:08:04,681\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:08:07 (running for 00:02:03.00)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          15</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:08:07,476\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:08:07,483\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m 2022-04-22 17:08:07,470\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64306, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64306, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64313, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7ff3dc6d2160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64306)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64313)\u001b[0m 2022-04-22 17:08:07,457\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64313, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7ff3dc6d2160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64313)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64313)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64313)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64313)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64313)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64313)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64313)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64313)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64313)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64313)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64313)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64313)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64313)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:08:08,688\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m 2022-04-22 17:08:11,532\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m 2022-04-22 17:08:11,532\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m 2022-04-22 17:08:11,532\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:08:14 (running for 00:02:09.96)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          16</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:08:14,434\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:08:14,436\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m 2022-04-22 17:08:14,429\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64318, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64318, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64321, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fd95be0f160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64318)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64321)\u001b[0m 2022-04-22 17:08:14,418\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64321, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fd95be0f160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64321)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64321)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64321)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64321)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64321)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64321)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64321)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64321)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64321)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64321)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64321)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64321)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64321)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:08:15,644\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m 2022-04-22 17:08:18,619\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m 2022-04-22 17:08:18,619\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m 2022-04-22 17:08:18,619\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:08:21 (running for 00:02:17.27)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          17</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:08:21,741\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:08:21,749\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m 2022-04-22 17:08:21,735\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64332, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64332, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64335, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7f83e67100>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64332)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64335)\u001b[0m 2022-04-22 17:08:21,719\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64335, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7f83e67100>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64335)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64335)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64335)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64335)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64335)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64335)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64335)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64335)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64335)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64335)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64335)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64335)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64335)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:08:22,786\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m 2022-04-22 17:08:25,856\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m 2022-04-22 17:08:25,856\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m 2022-04-22 17:08:25,856\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:08:28 (running for 00:02:24.45)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          18</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:08:28,919\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:08:28,925\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m 2022-04-22 17:08:28,914\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64342, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64342, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64347, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fd92c20a160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64342)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64347)\u001b[0m 2022-04-22 17:08:28,900\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64347, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fd92c20a160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64347)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64347)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64347)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64347)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64347)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64347)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64347)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64347)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64347)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64347)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64347)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64347)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64347)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:08:30,618\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m 2022-04-22 17:08:33,869\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m 2022-04-22 17:08:33,870\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m 2022-04-22 17:08:33,870\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:08:37 (running for 00:02:32.88)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          19</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:08:37,358\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:08:37,363\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m 2022-04-22 17:08:37,352\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64352, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64352, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64394, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fd15455f160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64352)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64394)\u001b[0m 2022-04-22 17:08:37,339\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64394, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fd15455f160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64394)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64394)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64394)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64394)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64394)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64394)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64394)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64394)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64394)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64394)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64394)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64394)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64394)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:08:38,816\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m 2022-04-22 17:08:42,266\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m 2022-04-22 17:08:42,266\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m 2022-04-22 17:08:42,266\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:08:45 (running for 00:02:40.79)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          20</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:08:45,260\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:08:45,265\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m 2022-04-22 17:08:45,254\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64399, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64399, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64403, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fbcc458f160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64399)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64403)\u001b[0m 2022-04-22 17:08:45,234\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64403, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fbcc458f160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64403)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64403)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64403)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64403)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64403)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64403)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64403)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64403)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64403)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64403)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64403)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64403)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64403)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:08:46,902\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m 2022-04-22 17:08:50,532\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m 2022-04-22 17:08:50,532\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m 2022-04-22 17:08:50,532\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:08:53 (running for 00:02:49.23)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          21</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:08:53,701\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:08:53,705\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m 2022-04-22 17:08:53,695\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64412, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64412, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64416, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f8e644aa160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64412)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64416)\u001b[0m 2022-04-22 17:08:53,674\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64416, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f8e644aa160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64416)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64416)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64416)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64416)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64416)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64416)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64416)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64416)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64416)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64416)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64416)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64416)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64416)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:08:54,719\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m 2022-04-22 17:08:57,815\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m 2022-04-22 17:08:57,815\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m 2022-04-22 17:08:57,815\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64426)\u001b[0m 2022-04-22 17:09:00,813\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64426, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fcc1b7ff160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64426)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64426)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64426)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64426)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64426)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64426)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64426)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64426)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64426)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64426)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64426)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64426)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64426)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m 2022-04-22 17:09:00,827\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64423, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64423, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64426, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fcc1b7ff160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64423)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:09:00 (running for 00:02:56.36)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          22</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:09:00,832\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:09:00,836\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "2022-04-22 17:09:01,751\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m 2022-04-22 17:09:04,930\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m 2022-04-22 17:09:04,930\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m 2022-04-22 17:09:04,930\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:09:07 (running for 00:03:03.49)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          23</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:09:07,966\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:09:07,969\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m 2022-04-22 17:09:07,959\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64459, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64459, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64477, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fe140400130>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64459)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64477)\u001b[0m 2022-04-22 17:09:07,948\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64477, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fe140400130>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64477)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64477)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64477)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64477)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64477)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64477)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64477)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64477)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64477)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64477)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64477)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64477)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64477)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:09:09,763\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m 2022-04-22 17:09:13,291\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m 2022-04-22 17:09:13,291\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m 2022-04-22 17:09:13,291\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64486)\u001b[0m 2022-04-22 17:09:16,439\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64486, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fc4ac337100>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64486)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64486)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64486)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64486)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64486)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64486)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64486)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64486)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64486)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64486)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64486)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64486)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64486)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:09:16 (running for 00:03:11.98)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          24</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:09:16,458\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:09:16,466\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m 2022-04-22 17:09:16,453\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64482, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64482, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64486, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fc4ac337100>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64482)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:09:17,773\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m 2022-04-22 17:09:20,963\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m 2022-04-22 17:09:20,963\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m 2022-04-22 17:09:20,963\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:09:23 (running for 00:03:19.52)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          25</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:09:23,998\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:09:24,004\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m 2022-04-22 17:09:23,992\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64530, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64530, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64533, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fab98652160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64530)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64533)\u001b[0m 2022-04-22 17:09:23,978\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64533, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fab98652160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64533)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64533)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64533)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64533)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64533)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64533)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64533)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64533)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64533)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64533)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64533)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64533)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64533)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:09:25,734\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m 2022-04-22 17:09:28,594\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m 2022-04-22 17:09:28,594\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m 2022-04-22 17:09:28,594\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:09:31 (running for 00:03:26.92)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          26</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:09:31,388\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:09:31,394\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m 2022-04-22 17:09:31,383\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64540, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64540, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64543, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f89447f0160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64540)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64543)\u001b[0m 2022-04-22 17:09:31,371\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64543, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f89447f0160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64543)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64543)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64543)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64543)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64543)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64543)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64543)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64543)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64543)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64543)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64543)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64543)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64543)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:09:32,650\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m 2022-04-22 17:09:35,978\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m 2022-04-22 17:09:35,978\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m 2022-04-22 17:09:35,978\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:09:39 (running for 00:03:34.54)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          27</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:09:39,018\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:09:39,029\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m 2022-04-22 17:09:39,012\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64548, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64548, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64555, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fec241ca160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64548)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64555)\u001b[0m 2022-04-22 17:09:38,990\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64555, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fec241ca160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64555)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64555)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64555)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64555)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64555)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64555)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64555)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64555)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64555)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64555)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64555)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64555)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64555)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:09:40,791\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m 2022-04-22 17:09:44,101\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m 2022-04-22 17:09:44,102\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m 2022-04-22 17:09:44,102\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:09:47 (running for 00:03:42.75)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          28</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:09:47,227\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:09:47,233\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m 2022-04-22 17:09:47,223\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64561, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64561, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64569, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f9ce41d3160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64561)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64569)\u001b[0m 2022-04-22 17:09:47,212\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64569, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f9ce41d3160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64569)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64569)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64569)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64569)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64569)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64569)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64569)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64569)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64569)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64569)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64569)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64569)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64569)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:09:48,843\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m 2022-04-22 17:09:52,187\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m 2022-04-22 17:09:52,187\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m 2022-04-22 17:09:52,187\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64580)\u001b[0m 2022-04-22 17:09:55,445\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64580, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f90ebd3b160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64580)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64580)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64580)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64580)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64580)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64580)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64580)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64580)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64580)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64580)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64580)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64580)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64580)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:09:55 (running for 00:03:50.99)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          29</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:09:55,462\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:09:55,468\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m 2022-04-22 17:09:55,457\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64577, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64577, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64580, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f90ebd3b160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64577)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:09:56,782\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m 2022-04-22 17:09:59,942\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m 2022-04-22 17:09:59,942\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m 2022-04-22 17:09:59,942\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:10:02 (running for 00:03:58.52)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          30</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:10:02,992\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:10:02,999\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m 2022-04-22 17:10:02,988\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64589, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64589, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64592, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fab7c75b160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64589)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64592)\u001b[0m 2022-04-22 17:10:02,974\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64592, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fab7c75b160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64592)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64592)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64592)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64592)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64592)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64592)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64592)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64592)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64592)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64592)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64592)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64592)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64592)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:10:04,791\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m 2022-04-22 17:10:07,904\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m 2022-04-22 17:10:07,904\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m 2022-04-22 17:10:07,904\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:10:10 (running for 00:04:06.45)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          31</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:10:10,922\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:10:10,933\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m 2022-04-22 17:10:10,917\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64640, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64640, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64644, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fa32c273160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64640)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64644)\u001b[0m 2022-04-22 17:10:10,897\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64644, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fa32c273160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64644)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64644)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64644)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64644)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64644)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64644)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64644)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64644)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64644)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64644)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64644)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64644)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64644)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:10:11,827\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m 2022-04-22 17:10:15,185\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m 2022-04-22 17:10:15,186\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m 2022-04-22 17:10:15,186\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:10:18 (running for 00:04:14.14)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          32</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:10:18,615\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m 2022-04-22 17:10:18,610\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64649, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64649, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64658, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fb45c1a0160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64649)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64658)\u001b[0m 2022-04-22 17:10:18,591\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64658, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fb45c1a0160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64658)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64658)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64658)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64658)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64658)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64658)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64658)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64658)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64658)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64658)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64658)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64658)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64658)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:10:18,621\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "2022-04-22 17:10:19,881\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m 2022-04-22 17:10:23,160\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m 2022-04-22 17:10:23,160\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m 2022-04-22 17:10:23,160\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:10:26 (running for 00:04:21.97)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          33</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:10:26,447\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:10:26,454\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m 2022-04-22 17:10:26,442\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64665, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64665, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64702, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fe83432b160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64665)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64702)\u001b[0m 2022-04-22 17:10:26,428\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64702, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fe83432b160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64702)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64702)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64702)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64702)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64702)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64702)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64702)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64702)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64702)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64702)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64702)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64702)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64702)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:10:27,869\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m 2022-04-22 17:10:30,754\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m 2022-04-22 17:10:30,754\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m 2022-04-22 17:10:30,754\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64712)\u001b[0m 2022-04-22 17:10:33,541\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64712, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f8520991160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64712)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64712)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64712)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64712)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64712)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64712)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64712)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64712)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64712)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64712)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64712)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64712)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64712)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m 2022-04-22 17:10:33,553\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64709, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64709, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64712, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f8520991160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64709)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:10:33 (running for 00:04:29.09)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          34</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:10:33,559\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:10:33,561\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "2022-04-22 17:10:34,766\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m 2022-04-22 17:10:37,782\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m 2022-04-22 17:10:37,782\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m 2022-04-22 17:10:37,782\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:10:40 (running for 00:04:36.10)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          35</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:10:40,572\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:10:40,574\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m 2022-04-22 17:10:40,566\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64719, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64719, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64722, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fc2c43e3160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64719)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64722)\u001b[0m 2022-04-22 17:10:40,553\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64722, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fc2c43e3160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64722)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64722)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64722)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64722)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64722)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64722)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64722)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64722)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64722)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64722)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64722)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64722)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64722)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:10:41,788\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m 2022-04-22 17:10:44,694\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m 2022-04-22 17:10:44,694\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m 2022-04-22 17:10:44,695\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:10:47 (running for 00:04:43.12)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          36</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:10:47,594\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:10:47,598\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m 2022-04-22 17:10:47,590\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64727, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64727, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64732, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fb35c77b160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64727)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64732)\u001b[0m 2022-04-22 17:10:47,574\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64732, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fb35c77b160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64732)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64732)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64732)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64732)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64732)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64732)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64732)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64732)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64732)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64732)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64732)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64732)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64732)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:10:48,767\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m 2022-04-22 17:10:51,798\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m 2022-04-22 17:10:51,798\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m 2022-04-22 17:10:51,798\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:10:55 (running for 00:04:50.55)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          37</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:10:55,022\tERROR trial_runner.py:876 -- Trial PPO_MergePOEnv-v0_05735_00000: Error processing event.\n",
      "NoneType: None\n",
      "2022-04-22 17:10:55,026\tINFO trial_runner.py:1239 -- Trial PPO_MergePOEnv-v0_05735_00000: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m 2022-04-22 17:10:55,016\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64767, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64767, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64776, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fd95441b160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64767)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64776)\u001b[0m 2022-04-22 17:10:55,003\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64776, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fd95441b160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64776)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64776)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64776)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64776)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64776)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64776)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64776)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64776)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64776)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64776)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64776)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64776)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64776)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:10:56,847\tINFO trial_runner.py:803 -- starting PPO_MergePOEnv-v0_05735_00000\n",
      "2022-04-22 17:10:58,880\tWARNING tune.py:650 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m 2022-04-22 17:11:00,202\tWARNING ppo.py:240 -- `train_batch_size` (100) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m 2022-04-22 17:11:00,202\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m 2022-04-22 17:11:00,202\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:11:03 (running for 00:04:59.22)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          38</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-22 17:11:03 (running for 00:04:59.23)<br>Memory usage on this node: 29.6/32.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/24.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/pbarde/ray_results/training_example<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MergePOEnv-v0_05735_00000</td><td style=\"text-align: right;\">          38</td><td>/Users/pbarde/ray_results/training_example/PPO_MergePOEnv-v0_05735_00000_0_2022-04-22_17-06-05/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m 2022-04-22 17:11:03,691\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64783, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 1035, in _init\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m     raise NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m NotImplementedError\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m \u001b[36mray::PPOTrainer.__init__()\u001b[39m (pid=64783, ip=127.0.0.1, repr=PPOTrainer)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 830, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/tune/trainable.py\", line 149, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 911, in setup\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 134, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m     remote_spaces = ray.get(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64786, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f9560a0b160>)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=64783)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64786)\u001b[0m 2022-04-22 17:11:03,681\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=64786, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f9560a0b160>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64786)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 493, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64786)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64786)\u001b[0m   File \"/Users/pbarde/code/flow/flow/utils/registry.py\", line 130, in create_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64786)\u001b[0m     return gym.envs.make(env_name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64786)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 156, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64786)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64786)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 101, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64786)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64786)\u001b[0m   File \"/opt/miniconda3/envs/flow2/lib/python3.8/site-packages/gym/envs/registration.py\", line 73, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64786)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64786)\u001b[0m   File \"/Users/pbarde/code/flow/flow/envs/merge.py\", line 76, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64786)\u001b[0m     raise KeyError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=64786)\u001b[0m KeyError: 'Environment parameter \"target_velocity\" not supplied'\n",
      "2022-04-22 17:11:03,920\tERROR tune.py:697 -- Trials did not complete: [PPO_MergePOEnv-v0_05735_00000]\n",
      "2022-04-22 17:11:03,922\tINFO tune.py:701 -- Total run time: 299.45 seconds (299.22 seconds for the tuning loop).\n",
      "2022-04-22 17:11:03,922\tWARNING tune.py:707 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    }
   ],
   "source": [
    "trials = run_experiments({\n",
    "    flow_params[\"exp_tag\"]: {\n",
    "        \"run\": alg_run,\n",
    "        \"env\": gym_name,\n",
    "        \"config\": {\n",
    "            **config\n",
    "        },\n",
    "        \"checkpoint_freq\": 1,  # number of iterations between checkpoints\n",
    "        \"checkpoint_at_end\": True,  # generate a checkpoint at the end\n",
    "        \"max_failures\": 999,\n",
    "        \"stop\": {  # stopping conditions\n",
    "            \"training_iteration\": 1,  # number of iterations to stop after\n",
    "        },\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Visualizing the results\n",
    "\n",
    "The simulation results are saved within the `ray_results/training_example` directory (we defined `training_example` at the start of this tutorial). The `ray_results` folder is by default located at your root `~/ray_results`. \n",
    "\n",
    "You can run `tensorboard --logdir=~/ray_results/training_example` (install it with `pip install tensorboard`) to visualize the different data outputted by your simulation.\n",
    "\n",
    "For more instructions about visualizing, please see `tutorial05_visualize.ipynb`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Restart from a checkpoint / Transfer learning\n",
    "\n",
    "If you wish to do transfer learning, or to resume a previous training, you will need to start the simulation from a previous checkpoint. To do that, you can add a `restore` parameter in the `run_experiments` argument, as follows:\n",
    "\n",
    "```python\n",
    "trials = run_experiments({\n",
    "    flow_params[\"exp_tag\"]: {\n",
    "        \"run\": alg_run,\n",
    "        \"env\": gym_name,\n",
    "        \"config\": {\n",
    "            **config\n",
    "        },\n",
    "        \"restore\": \"/ray_results/experiment/dir/checkpoint_50/checkpoint-50\"\n",
    "        \"checkpoint_freq\": 1,\n",
    "        \"checkpoint_at_end\": True,\n",
    "        \"max_failures\": 999,\n",
    "        \"stop\": {\n",
    "            \"training_iteration\": 1,\n",
    "        },\n",
    "    },\n",
    "})\n",
    "```\n",
    "\n",
    "The `\"restore\"` path should be such that the `[restore]/.tune_metadata` file exists.\n",
    "\n",
    "There is also a `\"resume\"` parameter that you can set to `True` if you just wish to continue the training from a previously saved checkpoint, in case you are still training on the same experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
